input {
    file {
        type => "fpServer"
        path => "/opt/logstash/data/hivecore/fpLog/ALL/fpServer.log"
        tags => [ "fpServer", "logback" ]
        sincedb_path => "/opt/logstash/logs/fp_sincedb" # sincedb会记录日志文件的inode信息及当前读了多少bytes,以确保logstash停掉重启后可接着之前的位置继续收集日志
                sincedb_write_interval => 10    # 写sincedb的频率,logstash停止时会自动将当前收集位置记录到sincedb
                # start_position => "beginning"  # 注意,一定要设为beginning,默认是end.因为如果在logstash停掉期间,日志被rotate了两次,意味着有一整个日志文件是完全没有sincedb记录,logstash重启后就直接从start_position开始读,所以一定要设置为beginning.对于有sincedb记录的,logstash会接着停掉时的sincedb记录的位置继续收集日志
                discover_interval => 10
                stat_interval => 1
     }
     file {
        type => "bpServer"
        path => "/opt/logstash/data/hivecore/bpLog/ALL/bpServer.log"
        tags => [ "bpServer", "logback" ]
        sincedb_path => "/opt/logstash/logs/bp_sincedb" # sincedb会记录日志文件的inode信息及当前读了多少bytes,以确保logstash停掉重启后可接着之前的位置继续收集日志
                sincedb_write_interval => 10  # 写sincedb的频率,logstash停止时会自动将当前收集位置记录到sincedb
                # start_position => "beginning"  # 注意,一定要设为beginning,默认是end.因为如果在logstash停掉期间,日志被rotate了两次,意味着有一整个日志文件是完全没有sincedb记录,logstash重启后就直接从start_position开始读,所以一定要设置为beginning.对于有sincedb记录的,logstash会接着停掉时的sincedb记录的位置继续收集日志
                discover_interval => 10
                stat_interval => 1
     }
    # ntag日志
    file {
        type => "ntag"
        path => "/opt/logstash/data/ntag/Business/log.txt"
        tags => [ "ntag"]
        sincedb_path => "/opt/logstash/logs/sincedb-ntag"  # sincedb会记录日志文件的inode信息及当前读了多少bytes,以确保logstash停掉重启后可接着之前的位置继续收集日志
                sincedb_write_interval => 10    # 写sincedb的频率,logstash停止时会自动将当前收集位置记录到sincedb
                # start_position => "beginning"  # 注意,一定要设为beginning,默认是end.因为如果在logstash停掉期间,日志被rotate了两次,意味着有一整个日志文件是完全没有sincedb记录,logstash重启后就直接从start_position开始读,所以一定要设置为beginning.对于有sincedb记录的,logstash会接着停掉时的sincedb记录的位置继续收集日志
                discover_interval => 10
                stat_interval => 1
     }
     # nump日志
     file {
        type => "nump"
        path => "/opt/logstash/data/nump/nump-*.log"
        tags => [ "nump"]
        sincedb_path => "/opt/logstash/logs/sincedb-nump"       # sincedb会记录日志文件的inode信息及当前读了多少bytes,以确保logstash停掉重启后可接着之前的位置继续收集日志
                sincedb_write_interval => 10    # 写sincedb的频率,logstash停止时会自动将当前收集位置记录到sincedb
                # start_position => "beginning"  # 注意,一定要设为beginning,默认是end.因为如果在logstash停掉期间,日志被rotate了两次,意味着有一整个日志文件是完全没有sincedb记录,logstash重启后就直接从start_position开始读,所以一定要设置为beginning.对于有sincedb记录的,logstash会接着停掉时的sincedb记录的位置继续收集日志
                discover_interval => 10
                stat_interval => 1
     }
     # solar日志
     file {
        type => "solar"
        path => "/opt/logstash/data/nebula/SOLAR/OperationLog/Operation.log"
        tags => [ "solar"]
        sincedb_path => "/opt/logstash/logs/sincedb-solar"      # sincedb会记录日志文件的inode信息及当前读了多少bytes,以确保logstash停掉重启后可接着之前的位置继续收集日志
                sincedb_write_interval => 10    # 写sincedb的频率,logstash停止时会自动将当前收集位置记录到sincedb
                # start_position => "beginning"  # 注意,一定要设为beginning,默认是end.因为如果在logstash停掉期间,日志被rotate了两次,意味着有一整个日志文件是完全没有sincedb记录,logstash重启后就直接从start_position开始读,所以一定要设置为beginning.对于有sincedb记录的,logstash会接着停掉时的sincedb记录的位置继续收集日志
                discover_interval => 10
                stat_interval => 1
     }
     # infoshare日志
     file {
             type => "infoshare"
             path => "/opt/logstash/data/infoshare/logs/audit.log"
             tags => [ "infoshare"]
             sincedb_path => "/opt/logstash/logs/sincedb-infoshare"      # sincedb会记录日志文件的inode信息及当前读了多少bytes,以确保logstash停掉重启后可接着之前的位置继续收集日志
                     sincedb_write_interval => 10    # 写sincedb的频率,logstash停止时会自动将当前收集位置记录到sincedb
                     # start_position => "beginning"  # 注意,一定要设为beginning,默认是end.因为如果在logstash停掉期间,日志被rotate了两次,意味着有一整个日志文件是完全没有sincedb记录,logstash重启后就直接从start_position开始读,所以一定要设置为beginning.对于有sincedb记录的,logstash会接着停掉时的sincedb记录的位置继续收集日志
                     discover_interval => 10
                     stat_interval => 1
          }

}

filter {
        if [type] == "ntag" {
                if [message] =~ "^\[id\]" {
                } else {
                        drop{}
                }
                grok {
                        match => { "message" => "\[(?<log_id>id)\] (?<log_level>.+) %{TIMESTAMP_ISO8601:log_time} (?<thread_name>.+?) \- (?<content>.*)" }
                        remove_field => [ "message" ]
                }
        }

         else if [type] == "nump" {
                if [message] =~ "^\[.*?\] FATAL" {
                } else {
                        drop{}
                }
                grok {
                        match => { "message" => "\[(?<log_id>.*?)\] (?<log_level>.+) %{TIMESTAMP_ISO8601:log_time} (?<system>\w+) \[(?<thread_name>.+?)\] (?<class_name>.+)\- (?<content>.*)" }
                        remove_field => [ "message" ]
                }
        }
         else if [type] == "solar" {
                if [message] =~ "^\[.*?\] INFO" {
                } else {
                        drop{}
                }
                grok {
                        match => { "message" => "\[(?<log_id>.*?)\] (?<log_level>.+) %{TIMESTAMP_ISO8601:log_time} (?<system>\w+) \[(?<thread_name>.+?)\] (?<class_name>.+)\- (?<content>.*)" }
                        remove_field => [ "message" ]
                }
        }
        else if [type] == "infoshare" {
                        if [message] =~ "^\[\] INFO" {
                        } else {
                              drop{}
                        }
                        grok {
                                match => { "message" => "\[(?<log_id>.*?)\] (?<log_level>.+) %{TIMESTAMP_ISO8601:log_time} (?<system>\w+) \[(?<thread_name>.+?)\] (?<class_name>.+) \- (?<content>.*) " }
                                remove_field => [ "message" ]
                        }
               }
        else {
                # 处理fp和bp的日志
                if [message] =~ "^\[.+?\]" {
                } else {
                        drop{}
                }

                grok {
                        match => { "message" => "\[(?<log_id>.*?)\] (?<log_level>.+)  %{TIMESTAMP_ISO8601:log_time} (?<system>\w+) \[(?<thread_name>.+?)\] (?<class_name>.+)\- (?<content>.*)" }
                        remove_field => [ "message" ]
                }
                # 过滤service日志
                if [class_name] == "c.s.j.s.l.s.PerformanceServiceAdvice" {
                        drop{}
                }
                # 特殊处理INFO级别日志
                if [log_level] =~ "INFO" {
                    # 抓取耗时
                    grok {
                        match => {"content" => ".*?\"UsedTime\":%{INT:log_usedTime}.*?"}
                    }
                    # 类型转换
                    mutate {
                            convert => ["log_usedTime", "integer"]
                    }
                    # 判断是否匹配成功, 如果没有匹配成功就直接过滤
                    if ![log_usedTime] {
                        drop{}
                    } else {
                        # 如果匹配成功了, 就过滤时间没有超过3s的INFO日志
                        if [log_usedTime] < 3000 {
                            drop{}
                        }
                    }
                }
                # 根据级别过滤日志
                if !([log_level] in ["INFO", "WARN", "ERROR", "FATAL"]) {
                        drop{}
                }
        }
        # 日志级别大写
        mutate {
                uppercase => [log_level]
        }
	grok {
		match => {"content" => ".*?tags\":\[\"(?<log_operate>.+?)\",* *\"*(?<contentId>.*?)\"*\].*?"}
		add_field => ["contentID_", "%{contentId}"]
		remove_field => [ "contentId" ]
	}
	if [contentID_] == "%{contentId}" {
		mutate {
			update => ["contentID_", "NULL"]
		}
	}
        date {
                match => [ "log_time", "yyyy-MM-dd HH:mm:ss.SSS", "ISO8601" ]
                timezone => "Asia/Shanghai"
                target => "@timestamp"
                remove_field => [ "log_time" ]
        }

        if [content] =~ "^\{.*?\}$" {
        }else {
                drop{}
        }
        json {
                source => "content"
                target => "jsoncontent"
                remove_field => [ "content" ]
        }

}

output {
        # stdout {codec=> rubydebug}
        if [type] == "ntag" {
                elasticsearch {
                       hosts => "172.16.131.37:17100"
                       index => "logstash-ntag-%{+YYYY.MM}"
                       document_type => "sobeylog"
		       #dan wei ying gai shi miao, wo cai de
		       timeout => 60
		       retry_max_interval => 20
                }
        } else if [type] == "nump" {
		#da yin ri zhi, kan kan shen me qing kuang
                #stdout {codec => rubydebug}
		elasticsearch {
                        hosts => "172.16.131.37:17100"
                        index => "logstash-nump-%{+YYYY.MM}"
                        document_type => "sobeylog"
                        timeout => 60
			retry_max_interval => 20
		 }
        } else if [type] == "solar" {
                elasticsearch {
                       hosts => "172.16.131.37:17100"
                       index => "logstash-solar-%{+YYYY.MM}"
                       document_type => "sobeylog"
               	       timeout => 60
		       retry_max_interval => 20
		 }
        }
        else if [type] == "infoshare" {
                        elasticsearch {
                               hosts => "172.16.131.37:17100"
                               index => "logstash-infoshare-%{+YYYY.MM}"
                               document_type => "sobeylog"
                       	       timeout => 60
        		       retry_max_interval => 20
            }
        }
        else {
                # 处理fp和bp的日志
                elasticsearch {
                       hosts => "172.16.131.37:17100"
                       index => "logstash-%{+YYYY.MM}"
                       document_type => "sobeylog"
                       timeout => 60
		       retry_max_interval => 20
		 }
        }
        # 处理操作日志
        if ([log_operate] =~ "OperationLog") or ([log_operate] =~ "operateLog") {
               elasticsearch {
                       hosts => "172.16.131.37:17100"
                       index => "logstash-operate-%{+YYYY.MM}"
                       document_type => "hivecore-operatelog"
                       timeout => 60
		       retry_max_interval => 20
		 }
        }
}
